{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "18ed01ab",
            "metadata": {},
            "source": [
                "# BeejX Leaf Disease Model Training\n",
                "**Professional Training Pipeline | Optimized for Google Colab**\n",
                "\n",
                "This notebook runs the BeejX professional training script. It handles:\n",
                "1. Environment Setup\n",
                "2. Dataset Organization\n",
                "3. MobileNetV2 Training\n",
                "4. TFLite Conversion for Android"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mount_drive",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 0.0 [OPTIONAL] Mount Google Drive (For Large Files)\n",
                "# Run this ONLY if you uploaded 'project_upload.zip' to Google Drive.\n",
                "import os\n",
                "from google.colab import drive\n",
                "\n",
                "if not os.path.exists('project_upload.zip'):\n",
                "    print(\"Mounting Google Drive...\")\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # Copy the file to current workspace with progress bar\n",
                "    print(\"Copying project_upload.zip from Drive... (This shows progress now)\")\n",
                "    !rsync -ah --progress \"/content/drive/MyDrive/project_upload.zip\" .\n",
                "    print(\"Copy Complete!\")\n",
                "else:\n",
                "    print(\"project_upload.zip already exists. Skipping Drive mount.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "zip_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 0.1 Initialize & Unzip\n",
                "import os\n",
                "# Check if we need to unzip\n",
                "if os.path.exists('project_upload.zip'):\n",
                "    print(\"Unzipping project... (This takes a few minutes for 6GB)\")\n",
                "    !unzip -q project_upload.zip\n",
                "    print(\"Unzip Complete!\")\n",
                "else:\n",
                "    print(\"project_upload.zip' not found. Please Upload it or Run Step 0.0 to copy from Drive.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c827fa6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Setup Environment\n",
                "# Install dependencies from the professional requirements file\n",
                "!pip install -r requirements.txt\n",
                "!pip install pyyaml"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "hotfix_patch",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1.1 [AUTO-FIX] Apply Stability Patches (Fix OOM & Bugs)\n",
                "# Run this to FIX the Memory Crash and AttributeError.\n",
                "import os\n",
                "\n",
                "# 1. Fix src/scripts/loader.py (Disable Cache to save RAM)\n",
                "loader_code = \"\"\"\n",
                "import tensorflow as tf\n",
                "import os\n",
                "import glob\n",
                "from typing import Tuple, List, Optional\n",
                "import yaml\n",
                "\n",
                "def load_config(config_path: str = \"configs/config.yaml\") -> dict:\n",
                "    with open(config_path, 'r') as f:\n",
                "        return yaml.safe_load(f)\n",
                "\n",
                "class BeejXDataLoader:\n",
                "    def __init__(self, config: dict):\n",
                "        self.config = config\n",
                "        self.img_size = tuple(config['model']['input_shape'][:2])\n",
                "        self.batch_size = config['training']['batch_size']\n",
                "        \n",
                "        self.augment_layers = tf.keras.Sequential([\n",
                "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
                "            tf.keras.layers.RandomRotation(config['augmentation']['rotation_range']),\n",
                "            tf.keras.layers.RandomZoom(config['augmentation']['zoom_range']),\n",
                "        ])\n",
                "\n",
                "    def get_local_dataset(self) -> Optional[tf.data.Dataset]:\n",
                "        data_dir = self.config['data']['local_data_dir']\n",
                "        classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
                "        if not classes:\n",
                "            print(\"No class folders found in data dir.\")\n",
                "            return None\n",
                "            \n",
                "        print(f\"Auto-detected {len(classes)} classes: {classes}\")\n",
                "        \n",
                "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "            data_dir, validation_split=self.config['training']['validation_split'],\n",
                "            subset=\"training\", seed=123, image_size=self.img_size,\n",
                "            batch_size=self.batch_size, labels='inferred', label_mode='int'\n",
                "        )\n",
                "        \n",
                "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
                "            data_dir, validation_split=self.config['training']['validation_split'],\n",
                "            subset=\"validation\", seed=123, image_size=self.img_size,\n",
                "            batch_size=self.batch_size, labels='inferred', label_mode='int' \n",
                "        )\n",
                "\n",
                "        class_names = train_ds.class_names\n",
                "\n",
                "        train_ds = train_ds.map(lambda x, y: (self.augment_layers(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
                "        train_ds = train_ds.map(lambda x, y: (x/255.0, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
                "        val_ds = val_ds.map(lambda x, y: (x/255.0, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
                "\n",
                "        # REMOVED .cache() to prevent RAM explosion. Streaming from disk is safer.\n",
                "        train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "        val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "\n",
                "        return train_ds, val_ds, class_names\n",
                "\n",
                "    def get_combined_dataset(self):\n",
                "        return self.get_local_dataset()\n",
                "\"\"\"\n",
                "\n",
                "with open('src/scripts/loader.py', 'w') as f:\n",
                "    f.write(loader_code)\n",
                "print(\"Fixed src/scripts/loader.py (Disabled Cache)\")\n",
                "\n",
                "# 2. Fix src/train.py (Memory Warning Fix + Import Fix)\n",
                "train_code = \"\"\"\n",
                "import os\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import yaml\n",
                "import glob\n",
                "from scripts.loader import BeejXDataLoader\n",
                "from models.mobilenet import build_mobilenet_model\n",
                "\n",
                "def load_config(path='configs/config.yaml'):\n",
                "    with open(path, 'r') as f:\n",
                "        return yaml.safe_load(f)\n",
                "\n",
                "def main():\n",
                "    print(\"=\"*50)\n",
                "    print(\"BeejX Leaf Disease Model - Professional Training Pipeline\")\n",
                "    print(\"=\"*50)\n",
                "    config = load_config()\n",
                "    loader = BeejXDataLoader(config)\n",
                "    datasets = loader.get_combined_dataset()\n",
                "\n",
                "    if datasets is None:\n",
                "        return\n",
                "        \n",
                "    train_ds, val_ds, class_names = datasets\n",
                "    num_classes = len(class_names)\n",
                "    print(f\"Found {num_classes} classes: {class_names}\")\n",
                "\n",
                "    model = build_mobilenet_model(num_classes, config)\n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['training']['learning_rate']),\n",
                "        loss='sparse_categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "\n",
                "    print(\"\\\\nCalculating class weights (Optimized for Colab)...\")\n",
                "    data_dir = config['data']['local_data_dir']\n",
                "    y_train_list = []\n",
                "    try:\n",
                "        for i, name in enumerate(class_names):\n",
                "             safe_name = name if os.path.isdir(os.path.join(data_dir, name)) else name\n",
                "             count = len(glob.glob(os.path.join(data_dir, safe_name, \"*\")))\n",
                "             y_train_list.extend([i] * int(count * 0.8))\n",
                "             \n",
                "        from sklearn.utils import class_weight\n",
                "        y_train = np.array(y_train_list)\n",
                "        weights = class_weight.compute_class_weight(\n",
                "            class_weight='balanced',\n",
                "            classes=np.unique(y_train),\n",
                "            y=y_train\n",
                "        )\n",
                "        class_weights = dict(enumerate(weights))\n",
                "        print(f\"Computed Class Weights (Fast): {class_weights}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Warning during weighting: {e}. Using equal weights.\")\n",
                "        class_weights = None\n",
                "\n",
                "    print(\"\\\\nStarting training...\")\n",
                "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
                "        os.path.join(config['paths']['output_dir'], \"best_model.keras\"),\n",
                "        save_best_only=True, monitor='val_accuracy'\n",
                "    )\n",
                "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
                "        monitor='val_loss', patience=3, restore_best_weights=True\n",
                "    )\n",
                "    \n",
                "    model.fit(\n",
                "        train_ds, epochs=config['training']['epochs'], \n",
                "        validation_data=val_ds, class_weight=class_weights,\n",
                "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
                "    )\n",
                "\n",
                "    print(\"\\\\nExporting to TFLite...\")\n",
                "    export_dir = config['paths']['output_dir']\n",
                "    os.makedirs(export_dir, exist_ok=True)\n",
                "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "    tflite_model = converter.convert()\n",
                "    with open(os.path.join(export_dir, \"model.tflite\"), \"wb\") as f:\n",
                "        f.write(tflite_model)\n",
                "    with open(os.path.join(export_dir, \"labels.txt\"), \"w\") as f:\n",
                "        for name in class_names:\n",
                "            f.write(name + \"\\\\n\")\n",
                "    print(\"SUCCESS! Model saved.\")\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n",
                "\"\"\"\n",
                "\n",
                "with open('src/train.py', 'w') as f:\n",
                "    f.write(train_code)\n",
                "print(\"Fixed src/train.py\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b7534be",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Verify GPU\n",
                "import tensorflow as tf\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")\n",
                "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
                "\n",
                "if not len(tf.config.list_physical_devices('GPU')) > 0:\n",
                "    print(\"WARNING: You are running on CPU. Enable GPU in Runtime > Change runtime type.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b69b3e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2.1 Prepare Data\n",
                "# Just upload your 'data' folder to the Colab file explorer\n",
                "# This script will organize it automatically.\n",
                "\n",
                "# Use src/scripts/organize.py because we moved it!\n",
                "!python src/scripts/organize.py\n",
                "print(\"Data organized successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Start Training Pipeline\n",
                "# This runs the professional script 'src/train.py'\n",
                "# It uses the settings in 'configs/config.yaml'\n",
                "\n",
                "!python src/train.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Download Model for Android\n",
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "if os.path.exists('exports/model.tflite'):\n",
                "    files.download('exports/model.tflite')\n",
                "    files.download('exports/labels.txt')\n",
                "else:\n",
                "    print(\"Model not found. Did training complete successfully?\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
